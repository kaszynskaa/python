# -*- coding: utf-8 -*-
"""EEG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TQTWeHvBG7b5UDxu9CSz4oxWb7mItusa
"""

# Install necessary packages
!pip install mne numpy matplotlib seaborn scikit-learn tensorflow nilearn

# Import statements
import numpy as np
import mne
from mne.preprocessing import ICA
from mne.time_frequency import tfr_multitaper, tfr_morlet
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.signal import welch
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout
from nilearn.connectome import ConnectivityMeasure

# Upload files manually
from google.colab import files
uploaded = files.upload()

# Load the data
file_name = list(uploaded.keys())[0]  # Get the uploaded file name
raw = mne.io.read_raw_eeglab(file_name, preload=True)

# Preprocessing: Filtering
raw.filter(1., 40., fir_design='firwin')

# Plot raw data
raw.plot(n_channels=10, duration=30, scalings='auto')

# Set up and fit the ICA
ica = ICA(n_components=15, random_state=97)
ica.fit(raw)
ica.plot_components()

# Find and remove artifacts
ica.exclude = [0, 1]  # indices of the components to exclude
ica.apply(raw)

# Inspect annotations
print("Annotations:", raw.annotations)

# Check if there are annotations
if not raw.annotations:
    # Create artificial events if there are no annotations
    sfreq = raw.info['sfreq']
    duration = raw.times[-1]  # Duration in seconds
    n_events = 20  # Number of events to create
    event_times = np.linspace(0, duration, n_events)
    # Alternate between two classes: 1 and 2
    events = np.array([[int(time * sfreq), 0, 1 if i % 2 == 0 else 2] for i, time in enumerate(event_times)], dtype=int)
    event_id = {'Class 1': 1, 'Class 2': 2}
else:
    # Extract events from annotations
    events, event_id = mne.events_from_annotations(raw)

print("Events:", events)
print("Event ID:", event_id)

# Check if events are correctly extracted
if events.size == 0:
    raise ValueError("No events found. Please check your annotations and event definitions.")

epochs = mne.Epochs(raw, events, event_id, tmin=-0.2, tmax=0.5, baseline=(None, 0), preload=True)
# Plot epochs
epochs.plot(n_channels=10, n_epochs=10, scalings='auto')

# Compute and plot ERP
erp = epochs.average()
erp.plot()

# Time-frequency analysis with Morlet wavelets
freqs = np.logspace(*np.log10([6, 30]), num=8)
n_cycles = freqs / 4.  # Using shorter wavelets
power = tfr_morlet(epochs, freqs=freqs, n_cycles=n_cycles, return_itc=False, decim=3, n_jobs=1)

# Plot time-frequency power
power.plot([0], baseline=(None, 0), mode='logratio', title='TFR power')

# Spectral analysis: Power spectral density using Welch's method
psds = []
sfreq = raw.info['sfreq']
for epoch in epochs:
    f, pxx = welch(epoch, sfreq, nperseg=256)
    psds.append(pxx)
psds = np.array(psds)
psds_mean = psds.mean(axis=0)

plt.plot(f, psds_mean.T)
plt.xlabel('Frequency (Hz)')
plt.ylabel('Power Spectral Density (dB)')
plt.title('Power Spectral Density')
plt.show()

# Connectivity analysis using Nilearn
connectivity_measure = ConnectivityMeasure(kind='correlation')
connectivity_matrix = connectivity_measure.fit_transform([epochs.get_data().reshape(len(epochs), -1)])[0]

# Plot connectivity matrix
plt.figure(figsize=(10, 8))
sns.heatmap(connectivity_matrix, annot=True, cmap='viridis')
plt.title('Connectivity Matrix (Correlation)')
plt.xlabel('Channel')
plt.ylabel('Channel')
plt.show()

# Time series analysis
epoch_data = epochs.get_data()
mean_epoch = epoch_data.mean(axis=0)
plt.plot(mean_epoch.T)
plt.title('Mean Epoch Time Series')
plt.xlabel('Time (ms)')
plt.ylabel('Amplitude')
plt.show()

# Machine learning: Feature extraction
X = psds.reshape(len(psds), -1)
y = epochs.events[:, -1]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Random forest classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

print("Random Forest Classification Report:\n", classification_report(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

# Deep learning: Simple CNN
X_cnn = epoch_data.transpose((0, 2, 1))  # Reshape for Conv1D: (samples, time, channels)
y_cnn = epochs.events[:, -1] - 1  # Adjust labels to be in the range [0, 1]

# Ensure y_cnn contains labels in the range [0, num_classes-1]
unique_classes = np.unique(y_cnn)
num_classes = len(unique_classes)
print(f"Unique classes in y_cnn: {unique_classes}")

X_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn = train_test_split(X_cnn, y_cnn, test_size=0.2, random_state=42)

model = Sequential([
    Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train_cnn.shape[1], X_train_cnn.shape[2])),
    MaxPooling1D(pool_size=2),
    Conv1D(64, kernel_size=3, activation='relu'),
    MaxPooling1D(pool_size=2),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(num_classes, activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

model.fit(X_train_cnn, y_train_cnn, epochs=10, batch_size=32, validation_data=(X_test_cnn, y_test_cnn))

# Evaluate the model
loss, accuracy = model.evaluate(X_test_cnn, y_test_cnn)
print(f"Test accuracy: {accuracy:.4f}")